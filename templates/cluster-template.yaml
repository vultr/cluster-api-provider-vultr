---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 172.25.0.0/16
    services:
      cidrBlocks:
        - 172.26.0.0/16
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1
    kind: VultrCluster
    name: "${CLUSTER_NAME}"
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VultrCluster
metadata:
  name: "${CLUSTER_NAME}"
spec:
  region: "${REGION}"
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: "${KUBERNETES_VERSION}"
  machineTemplate:
    infrastructureRef:
      kind: VultrMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: "${CLUSTER_NAME}-control-plane"
  kubeadmConfigSpec:
    initConfiguration:
      nodeRegistration:
        # We have to set the criSocket to containerd as kubeadm defaults to Vultr runtime if both containerd and Vultr sockets are found
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cgroup-driver: systemd
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
          cloud-provider: external
          provider-id: vultr://'{{ ds.meta_data["instance_id"] }}'
    clusterConfiguration:
      controllerManager:
        extraArgs: { enable-hostpath-provisioner: 'true' }
    joinConfiguration:
      nodeRegistration:
        # We have to set the criSocket to containerd as kubeadm defaults to Vultr runtime if both containerd and Vultr sockets are found
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cgroup-driver: systemd
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
---
kind: VultrMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  template:
    spec:
      region: "${REGION}"
      planID: "${CONTROL_PLANE_PLANID}"
      vpc_id: "${VPCID}"
      snapshot_id: "${MACHINE_IMAGE}"
      sshKey: 
        - "${SSHKEY_ID}"

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels:
  template:
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: "${KUBERNETES_VERSION}"
      bootstrap:
        configRef:
          name: "${CLUSTER_NAME}-md-0"
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: "${CLUSTER_NAME}-md-0"
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: VultrMachineTemplate
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VultrMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      region: "${REGION}"
      planID: "${WORKER_PLANID}"
      vpc_id: "${VPCID}"
      snapshot_id: "${MACHINE_IMAGE}"
      sshKey: 
        - "${SSHKEY_ID}"
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          # We have to set the criSocket to containerd as kubeadm defaults to Vultr runtime if both containerd and Vultr sockets are found
          criSocket: unix:///var/run/containerd/containerd.sock
          kubeletExtraArgs:
            cloud-provider: external
            cgroup-driver: systemd
            eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
